{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following https://arxiv.org/pdf/1906.00363.pdf, we try to compute the predictability score of a sentence based on the probabilities of its component tokens. We use a language model trained on a large corpora of literary text and want to use the predictability score as a proxy for literary \"inventivity\": i.e. if the model can predict the various tokens in the sentence with high probability, then the sentence must be hackeneyed or cliché. \n",
    "\n",
    "The formula given in the paper is:\n",
    "\n",
    "$p(w_{1}|w_{2},w_{3},...,w_{n})p(w_{2}|w_{1},w_{3},...,w_{n})...p(w_{n}|w_{1},w_{2},...,w_{n-1})^{-1/n} =(\\prod_{i=1}^{n}p(w_{i}|w_{1},...,w_{i-1},w_{i+1},...,w_{n}))^{-1/n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing \n",
    "\n",
    "First we'll define our preprocessing function (applied prior to the model tokenizer for better handling of korean morphosyntactical features), then we'll load our model pre-trained on North Korean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "def preproc(sentence):\n",
    "    return ' '.join([_ for _ in komoran.morphs(sentence)]).replace('MASK', '[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0908 19:21:52.470256 37080 configuration_utils.py:262] loading configuration file ./jobert\\config.json\n",
      "I0908 19:21:52.524113 37080 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 20839\n",
      "}\n",
      "\n",
      "I0908 19:21:52.539073 37080 tokenization_utils_base.py:1167] Model name './jobert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './jobert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0908 19:21:52.736546 37080 tokenization_utils_base.py:1197] Didn't find file ./jobert\\added_tokens.json. We won't load it.\n",
      "I0908 19:21:52.742537 37080 tokenization_utils_base.py:1197] Didn't find file ./jobert\\tokenizer.json. We won't load it.\n",
      "I0908 19:21:52.744525 37080 tokenization_utils_base.py:1252] loading file ./jobert\\vocab.txt\n",
      "I0908 19:21:52.745522 37080 tokenization_utils_base.py:1252] loading file None\n",
      "I0908 19:21:52.747517 37080 tokenization_utils_base.py:1252] loading file ./jobert\\special_tokens_map.json\n",
      "I0908 19:21:52.748514 37080 tokenization_utils_base.py:1252] loading file ./jobert\\tokenizer_config.json\n",
      "I0908 19:21:52.749512 37080 tokenization_utils_base.py:1252] loading file None\n",
      "C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\transformers\\modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "I0908 19:21:55.488203 37080 configuration_utils.py:262] loading configuration file ./jobert\\config.json\n",
      "I0908 19:21:55.490197 37080 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 20839\n",
      "}\n",
      "\n",
      "I0908 19:21:55.491194 37080 modeling_utils.py:665] loading weights file ./jobert\\pytorch_model.bin\n",
      "W0908 19:22:01.939983 37080 modeling_utils.py:757] Some weights of the model checkpoint at ./jobert were not used when initializing BertForMaskedLM: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "I0908 19:22:01.940981 37080 modeling_utils.py:774] All the weights of BertForMaskedLM were initialized from the model checkpoint at ./jobert.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./jobert\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"./jobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute the probability of a single word with BERT's masked language modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_proba(sequence, word):\n",
    "    global model, tokenizer\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(input_ids)[0]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    mask_token_logits = torch.softmax(mask_token_logits, dim=1)\n",
    "    sought_after_token = word\n",
    "    sought_after_token_id = tokenizer.encode(sought_after_token, add_special_tokens=False, add_prefix_space=True)[0]\n",
    "    token_score = mask_token_logits[:, sought_after_token_id]\n",
    "    return token_score.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended to compute the probability of each word in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_by_word_proba(sequence):\n",
    "    global tokenizer\n",
    "    preprocessed_sequence = preproc(sequence)\n",
    "    word_dict = {}\n",
    "    for token in preprocessed_sequence.split(' '):\n",
    "        masked_sequence = preprocessed_sequence.replace(token, tokenizer.mask_token)\n",
    "        word_dict[token] = compute_word_proba(masked_sequence, token)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'아무것': 0.018641446,\n",
       " '도': 0.99329877,\n",
       " '가늠': 0.002576774,\n",
       " '하': 0.97214746,\n",
       " 'ㄹ': 0.99953544,\n",
       " '수': 0.9890463,\n",
       " '없': 0.9403979,\n",
       " '도록': 0.0008625049,\n",
       " '몸': 0.08521192,\n",
       " '과': 0.37629688,\n",
       " '마음': 0.17802098,\n",
       " '이': 0.9546155,\n",
       " '굳어지': 0.0019267205,\n",
       " '어': 0.9935823,\n",
       " '버': 0.016200697,\n",
       " '린': 0.0010652286,\n",
       " '속': 0.016586062,\n",
       " '에': 0.5310471,\n",
       " '한가지': 0.002677068,\n",
       " '생각': 0.13339023,\n",
       " '끌': 2.1601047e-05,\n",
       " '날': 0.00035557462,\n",
       " '처럼': 0.0044740182,\n",
       " '머리': 0.123638384,\n",
       " '으로': 0.012388377,\n",
       " '비끼': 0.010278981,\n",
       " '들': 0.3685645,\n",
       " '었': 0.9988689,\n",
       " '다': 0.9634075}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_word_by_word_proba('아무것도  가늠할수  없도록  몸과  마음이  굳어져버 린속에  한가지  생각이  끌날처럼  머리속으로  비껴  들었다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And averaging out the individual probabilities for a sentence score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04743360799931327"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def geometric_mean(series):\n",
    "    return np.array(series).prod()**(1.0/len(series))\n",
    "\n",
    "def compute_sentence_score(sentence):\n",
    "    return geometric_mean(list(compute_word_by_word_proba(sentence).values()))\n",
    "    \n",
    "compute_sentence_score('아무것도  가늠할수  없도록  몸과  마음이  굳어져버 린속에  한가지  생각이  끌날처럼  머리속으로  비껴  들었다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a test with a sentence full of cliches and one with a creative metaphor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliche = '어둠속에  빛나는  그윽한  눈동자 에  웬  불빛이  어리여  불꽃처럼  아름답게  반짝이 고있었다.'\n",
    "original = '단풍잎같은 손이 물을 연신 퍼서 뿌려댄다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliche sentence score:  0.03292289714058116\n",
      "Original sentence score:  0.015907948372126945\n"
     ]
    }
   ],
   "source": [
    "print('Cliche sentence score: ', compute_sentence_score(cliche))\n",
    "print('Original sentence score: ', compute_sentence_score(original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
